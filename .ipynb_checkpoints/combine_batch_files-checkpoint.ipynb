{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745961d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba976ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Spectre Files\n",
      "Folder '20250115_094145' has not been executed. Hence Vaild\n",
      "Folder '20250115_094546' has already been executed. Hence Skipping\n",
      "Folder '20250115_094938' has already been executed. Hence Skipping\n",
      "Folder '20250115_095356' has not been executed. Hence Vaild\n",
      "Folder '20250115_095742' has not been executed. Hence Vaild\n",
      "Folder '20250115_110130' has not been executed. Hence Vaild\n",
      "Folder '20250115_111214' has already been executed. Hence Skipping\n",
      "Folder '20250115_112038' has already been executed. Hence Skipping\n",
      "Folder '20250115_112924' has already been executed. Hence Skipping\n",
      "Folder '20250115_113759' has not been executed. Hence Vaild\n",
      "Folder '20250115_115453' has already been executed. Hence Skipping\n",
      "Folder '20250115_120432' has already been executed. Hence Skipping\n",
      "Folder '20250115_121308' has already been executed. Hence Skipping\n",
      "Folder '20250115_122316' has already been executed. Hence Skipping\n",
      "Folder '20250115_123158' has already been executed. Hence Skipping\n",
      "Folder '20250116_152041' has not been executed. Hence Vaild\n",
      "Folder '20250116_155218' has not been executed. Hence Vaild\n",
      "Folder '20250116_160852' has already been executed. Hence Skipping\n",
      "Folder '20250116_162935' has not been executed. Hence Vaild\n",
      "Folder '20250116_164414' has already been executed. Hence Skipping\n",
      "Folder '20250116_165921' has not been executed. Hence Vaild\n",
      "Proxessing Folder:  20250115_094145  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250115_094145.csv\n",
      "Proxessing Folder:  20250115_094145  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250115_094145' to executed_folders.txt\n",
      "Proxessing Folder:  20250115_095356  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250115_095356.csv\n",
      "Proxessing Folder:  20250115_095356  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250115_095356' to executed_folders.txt\n",
      "Proxessing Folder:  20250115_095742  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250115_095742.csv\n",
      "Proxessing Folder:  20250115_095742  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250115_095742' to executed_folders.txt\n",
      "Proxessing Folder:  20250115_110130  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250115_110130.csv\n",
      "Proxessing Folder:  20250115_110130  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250115_110130' to executed_folders.txt\n",
      "Proxessing Folder:  20250115_113759  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250115_113759.csv\n",
      "Proxessing Folder:  20250115_113759  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250115_113759' to executed_folders.txt\n",
      "Proxessing Folder:  20250116_152041  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250116_152041.csv\n",
      "Proxessing Folder:  20250116_152041  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250116_152041' to executed_folders.txt\n",
      "Proxessing Folder:  20250116_155218  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250116_155218.csv\n",
      "Proxessing Folder:  20250116_155218  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250116_155218' to executed_folders.txt\n",
      "Proxessing Folder:  20250116_162935  for AttackType:  Spectre\n",
      "Master DataFrame saved to ../data/combined data files/Spectre\\Spectre_20250116_162935.csv\n",
      "Proxessing Folder:  20250116_162935  for AttackType:  Spectre Completed Sucessfully\n",
      "Added '20250116_162935' to executed_folders.txt\n",
      "Proxessing Folder:  20250116_165921  for AttackType:  Spectre\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reduce() of empty iterable with no initial value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 176\u001b[0m\n\u001b[0;32m    174\u001b[0m isIdle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing Spectre Files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 176\u001b[0m processBatches(isIdle)\n\u001b[0;32m    178\u001b[0m isIdle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing Idle Files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 165\u001b[0m, in \u001b[0;36mprocessBatches\u001b[1;34m(isIdle)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProxessing Folder: \u001b[39m\u001b[38;5;124m\"\u001b[39m,folder_name,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for AttackType: \u001b[39m\u001b[38;5;124m\"\u001b[39m,attackType)\n\u001b[0;32m    164\u001b[0m clean_batches \u001b[38;5;241m=\u001b[39m loadAndCleanBatchFiles(batchPath, batch_files)\n\u001b[1;32m--> 165\u001b[0m master_df \u001b[38;5;241m=\u001b[39m generateMasterDataFrame(clean_batches)\n\u001b[0;32m    167\u001b[0m fileName \u001b[38;5;241m=\u001b[39m attackType\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m folder_name\n\u001b[0;32m    168\u001b[0m saveMasterDataFrame(master_df,attackType,fileName)\n",
      "Cell \u001b[1;32mIn[2], line 123\u001b[0m, in \u001b[0;36mgenerateMasterDataFrame\u001b[1;34m(cleaned_up_batch_dataframes)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerateMasterDataFrame\u001b[39m(cleaned_up_batch_dataframes):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# Merge all DataFrames on the 'index' column using an outer join to keep all rows\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cleaned_up_batch_dataframes\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 123\u001b[0m     master_df \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m left, right: pd\u001b[38;5;241m.\u001b[39mmerge(left, right, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m), dataframes)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Sort by 'index' column and reset the DataFrame index\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     master_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: reduce() of empty iterable with no initial value"
     ]
    }
   ],
   "source": [
    "\n",
    "idle_file_directory = \"../data/idle/\"\n",
    "attack_file_directory = \"../data/spectre/\"\n",
    "\n",
    "combined_directory = \"../data/combined data files/\"\n",
    "\n",
    "idle_run_folders = [d for d in os.listdir(idle_file_directory) if os.path.isdir(os.path.join(idle_file_directory, d))]\n",
    "attack_run_folders = [d for d in os.listdir(attack_file_directory) if os.path.isdir(os.path.join(attack_file_directory, d))]\n",
    "\n",
    "\n",
    "\n",
    "def getBatchFileInDirectory(path,folder):\n",
    "    finalpath = os.path.join(path,folder,\"csv\")\n",
    "    if os.path.isdir(finalpath):\n",
    "        # Get the list of files in the final path\n",
    "        batch_files = [f for f in os.listdir(finalpath) if os.path.isfile(os.path.join(finalpath, f))]\n",
    "        return batch_files\n",
    "    else:\n",
    "        # Return an empty list if the path does not exist or is not a directory\n",
    "        return []\n",
    "\n",
    "def getExecutedFolders(file_path):\n",
    "    executed_folders = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read each line and strip any whitespace or newline characters\n",
    "            executed_folders = [line.strip() for line in file if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except IOError:\n",
    "        print(f\"An error occurred while reading the file {file_path}.\")\n",
    "    return executed_folders\n",
    "    \n",
    "def getValidFolders(folders,isIdle):\n",
    "    valid_folders = []\n",
    "    executed_folders_path = (idle_file_directory if isIdle else attack_file_directory) + \"executed_folders.txt\"\n",
    "    executed_folders = getExecutedFolders(executed_folders_path)\n",
    "    for folder in folders:\n",
    "        if folder in executed_folders:\n",
    "            print(f\"Folder '{folder}' has already been executed. Hence Skipping\")\n",
    "        else:\n",
    "            print(f\"Folder '{folder}' has not been executed. Hence Vaild\")\n",
    "            valid_folders.append(folder)\n",
    "    \n",
    "    return valid_folders\n",
    "\n",
    "\n",
    "def getValidBatchFiles(isIdle):\n",
    "    # Determine folders and path based on isIdle\n",
    "    folders = idle_run_folders if isIdle else attack_run_folders\n",
    "    path = idle_file_directory if isIdle else attack_file_directory\n",
    "\n",
    "    # Get the valid folders that haven't been executed\n",
    "    valid_folders = getValidFolders(folders, isIdle)\n",
    "\n",
    "    # Initialize a list to store batch paths and files as key-value pairs\n",
    "    validBatchFiles = []\n",
    "\n",
    "    # Iterate over each valid folder\n",
    "    for folder in valid_folders:\n",
    "        # Get batch files and batch path\n",
    "        batch_files = getBatchFileInDirectory(path, folder)\n",
    "        batchPath = os.path.join(path, folder, \"csv\")\n",
    "\n",
    "        # Store batch path and files as a dictionary in validBatchFiles\n",
    "        validBatchFiles.append({batchPath: batch_files})\n",
    "\n",
    "    return validBatchFiles\n",
    "\n",
    "def validBatchPath(batchPath, batch_files):\n",
    "    # Generate full paths by joining batchPath with each file name in batch_files\n",
    "    complete_paths = [os.path.join(batchPath, file) for file in batch_files]\n",
    "    return complete_paths\n",
    "\n",
    "def loadBatchFilesAsDataFrames(batchPath, batch_files):\n",
    "    batch_dataframes = {}\n",
    "    batch_count = 1  # Counter to label batch files sequentially\n",
    "\n",
    "    for file in batch_files:\n",
    "        file_path = os.path.join(batchPath, file)\n",
    "        \n",
    "        # Check if the file is a default file\n",
    "        if \"default\" in file:\n",
    "            key = \"default\"\n",
    "        # Check if the file is a batch file and assign a batch key sequentially\n",
    "        elif \"batch\" in file:\n",
    "            key = f\"batch_{batch_count}\"\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            # Skip files that do not match the expected naming convention\n",
    "            continue\n",
    "\n",
    "        # Load the CSV file as a DataFrame and add it to the dictionary\n",
    "        batch_dataframes[key] = pd.read_csv(file_path)\n",
    "\n",
    "    return batch_dataframes\n",
    "\n",
    "def cleanupBatchFile(df):\n",
    "    # Step 1: Drop columns where all values are zero\n",
    "    df = df.loc[:, (df != 0).any(axis=0)].copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Step 2: Reset the index column to start at 0 and retain increments\n",
    "    index_column_name = df.columns[0]\n",
    "    df.loc[:, index_column_name] = df[index_column_name] - df[index_column_name].iloc[0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def loadAndCleanBatchFiles(batchPath, batch_files):\n",
    "    # Load the batch files using the provided function\n",
    "    batch_dataframes = loadBatchFilesAsDataFrames(batchPath, batch_files)\n",
    "    \n",
    "    # Dictionary to store cleaned DataFrames\n",
    "    cleaned_up_batch_dataframes = {}\n",
    "    \n",
    "    # Apply cleaning to each loaded DataFrame\n",
    "    for key, df in batch_dataframes.items():\n",
    "        cleaned_up_batch_dataframes[key] = cleanupBatchFile(df)\n",
    "    \n",
    "    return cleaned_up_batch_dataframes\n",
    "\n",
    "def generateMasterDataFrame(cleaned_up_batch_dataframes):\n",
    "    # Merge all DataFrames on the 'index' column using an outer join to keep all rows\n",
    "    dataframes = list(cleaned_up_batch_dataframes.values())\n",
    "    master_df = reduce(lambda left, right: pd.merge(left, right, on='index', how='outer'), dataframes)\n",
    "    \n",
    "    # Sort by 'index' column and reset the DataFrame index\n",
    "    master_df.sort_values(by='index', inplace=True)\n",
    "    master_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    master_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Convert all columns to integers\n",
    "    master_df = master_df.astype(int)\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def saveMasterDataFrame(master_df, attack_type, filename):\n",
    "    # Create the output path\n",
    "    output_path = os.path.join(combined_directory, attack_type, f\"{filename}.csv\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create directories if not exist\n",
    "    \n",
    "    # Save the master DataFrame to the specified CSV location\n",
    "    master_df.to_csv(output_path, index=False)\n",
    "    print(f\"Master DataFrame saved to {output_path}\")\n",
    "    \n",
    "def addFolderNameToExecutedFolders(executed_folders_path, folder_name):\n",
    "    with open(executed_folders_path, 'a') as file:\n",
    "        file.write(f\"{folder_name}\\n\")\n",
    "    print(f\"Added '{folder_name}' to executed_folders.txt\")\n",
    "    \n",
    "def processBatches(isIdle):\n",
    "    valid_batch_files = getValidBatchFiles(isIdle)\n",
    "\n",
    "    for batch_dict in valid_batch_files:\n",
    "        for batchPath, batch_files in batch_dict.items():\n",
    "\n",
    "            parent_path = os.path.dirname(batchPath)\n",
    "            # Extract the folder name\n",
    "            folder_name = os.path.basename(parent_path)\n",
    "\n",
    "            attackType = \"Idle\" if isIdle else \"Spectre\"\n",
    "\n",
    "            print(f\"Proxessing Folder: \",folder_name,\" for AttackType: \",attackType)\n",
    "            clean_batches = loadAndCleanBatchFiles(batchPath, batch_files)\n",
    "            master_df = generateMasterDataFrame(clean_batches)\n",
    "\n",
    "            fileName = attackType+\"_\" + folder_name\n",
    "            saveMasterDataFrame(master_df,attackType,fileName)\n",
    "            print(f\"Proxessing Folder: \",folder_name,\" for AttackType: \",attackType,\"Completed Sucessfully\")\n",
    "            executed_folders_path = (idle_file_directory if isIdle else attack_file_directory) + \"executed_folders.txt\"\n",
    "            addFolderNameToExecutedFolders(executed_folders_path, folder_name)\n",
    "\n",
    "            \n",
    "isIdle = False\n",
    "print(\"processing Spectre Files\")\n",
    "processBatches(isIdle)\n",
    "\n",
    "isIdle = True\n",
    "\n",
    "\n",
    "print(\"processing Idle Files\")\n",
    "processBatches(isIdle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a1f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab9cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
