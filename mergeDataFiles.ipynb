{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753f26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651c2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idle_directory = \"../data/combined data files/idle/\"\n",
    "attack_directory = \"../data/combined data files/spectre/\"\n",
    "\n",
    "idle_files = [file for file in os.listdir(idle_directory) if file.endswith('.csv')]\n",
    "attack_files = [file for file in os.listdir(attack_directory) if file.endswith('.csv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "715f05a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 0, 19, 0, 22]\n"
     ]
    }
   ],
   "source": [
    "DEFAULT = ['Branch Instructions', 'Branch Misses', 'Cache References', 'Cache Misses']\n",
    "BATCH1 = ['L1 Data Cache Loads', 'L1 Data Cache Load Misses',\n",
    "          'Executed Branch Instructions', 'Executed Conditional Branches']\n",
    "BATCH2 = ['L2 All Demand Data Reads', 'L2 Demand Data Read Hits',\n",
    "          'Retired Branch Instructions', 'Retired Conditional Branches']\n",
    "BATCH3 = ['Offcore Demand Data Reads LLC Miss to DRAM',\n",
    "          'Offcore Demand Data Reads LLC Hit Any Response',\n",
    "          'Offcore All Data Reads LLC Any Response',\n",
    "          'Offcore All Data Reads LLC Miss to DRAM']\n",
    "BATCH4 = ['Retired Branch Mispredictions', 'Retired Near-Taken Branch Mispredictions',\n",
    "          'Executed Branch Mispredictions', 'Executed Conditional Branch Mispredictions']\n",
    "\n",
    "fileName = idle_directory + idle_files[0]\n",
    "df = pd.read_csv(fileName)\n",
    "\n",
    "def checkedBatch(batch, droppedBatches):\n",
    "    default_zero = (batch[DEFAULT] == 0).all(axis=0).sum()\n",
    "    batch1_zero = (batch[BATCH1] == 0).all(axis=0).sum()\n",
    "    batch2_zero = (batch[BATCH2] == 0).all(axis=0).sum()\n",
    "    batch3_zero = (batch[BATCH3] == 0).all(axis=0).sum()\n",
    "    batch4_zero = (batch[BATCH4] == 0).all(axis=0).sum()\n",
    "\n",
    "    if default_zero == 4:\n",
    "        droppedBatches[0] += 1\n",
    "        return False\n",
    "    elif batch1_zero  == 4:\n",
    "        droppedBatches[1] += 1\n",
    "        return False\n",
    "    elif batch2_zero  == 4:\n",
    "        droppedBatches[2] += 1\n",
    "        return False\n",
    "    elif batch3_zero == 4:\n",
    "        droppedBatches[3] += 1\n",
    "        return False\n",
    "    elif batch4_zero == 4:\n",
    "        droppedBatches[4] += 1\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Initialize droppedBatches counter\n",
    "droppedBatches = [0, 0, 0, 0, 0]\n",
    "\n",
    "# Iterate through DataFrame in chunks of 5 rows\n",
    "for i in range(100):\n",
    "    batch = df[i * 5:(i + 1) * 5]\n",
    "    checkedBatch(batch, droppedBatches)\n",
    "\n",
    "# Print results\n",
    "print(droppedBatches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d0a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate a progress bar\n",
    "def progress_bar(current, total, bar_length=50):\n",
    "    progress = current / total\n",
    "    bar = '=' * int(progress * bar_length) + '-' * (bar_length - int(progress * bar_length))\n",
    "    sys.stdout.write(f'\\r[{bar}] {progress * 100:.2f}%')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Function to combine and save rows in batches of specified size\n",
    "def combine_in_batches(files, isIdle, output_file, batch_size):\n",
    "    droppedBatches = [0, 0, 0, 0, 0]  # Keeps track of dropped batches for each category\n",
    "    totalBatches = 0  # Keeps track of the total batches\n",
    "\n",
    "    batch_data = []  # To store valid batches\n",
    "\n",
    "    total_files = len(files)  # Total number of files to process\n",
    "    print(\"Processing Files:\")\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        # Show progress bar\n",
    "        progress_bar(idx + 1, total_files)\n",
    "\n",
    "        if isIdle:\n",
    "            fileName = idle_directory + file\n",
    "        else:\n",
    "            fileName = attack_directory + file\n",
    "\n",
    "        df = pd.read_csv(fileName)  # Read the CSV file\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]  # Get a batch of rows\n",
    "            if len(batch) == batch_size:  # Only include full batches\n",
    "                totalBatches += 1  # Increment total batches\n",
    "                if checkedBatch(batch, droppedBatches):  # Check if the batch is valid\n",
    "                    batch_data.append(batch)\n",
    "\n",
    "    # Concatenate all valid full batches\n",
    "    combined_df = pd.concat(batch_data, ignore_index=True)\n",
    "    combined_df.to_csv(output_file, index=False)  # Save to output file\n",
    "\n",
    "    # Analysis: Print total and dropped batch details\n",
    "    print(\"\\nAnalysis of Batches:\")\n",
    "    print(f\"Total Batches Processed: {totalBatches}\")\n",
    "    print(f\"Dropped Batches by Category:\")\n",
    "    print(f\"  DEFAULT: {droppedBatches[0]}\")\n",
    "    print(f\"  BATCH1: {droppedBatches[1]}\")\n",
    "    print(f\"  BATCH2: {droppedBatches[2]}\")\n",
    "    print(f\"  BATCH3: {droppedBatches[3]}\")\n",
    "    print(f\"  BATCH4: {droppedBatches[4]}\")\n",
    "    print(f\"Valid Batches Included: {totalBatches - sum(droppedBatches)}\")\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b662bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 48534\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 15809\n",
      "  BATCH1: 134\n",
      "  BATCH2: 10463\n",
      "  BATCH3: 0\n",
      "  BATCH4: 7174\n",
      "Valid Batches Included: 14954\n",
      "Idle files combined into '../data/combined data files/combined_idle_batches.csv'\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 52031\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 12678\n",
      "  BATCH1: 1011\n",
      "  BATCH2: 14110\n",
      "  BATCH3: 678\n",
      "  BATCH4: 5409\n",
      "Valid Batches Included: 18145\n",
      "Attack files combined into '../data/combined data files/combined_attack_batches.csv'\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "\n",
    "# Combine idle files with a batch size of 5 and save\n",
    "idle_combined_df = combine_in_batches(idle_files, True, \"../data/combined data files/combined_idle_batches.csv\", batch_size)\n",
    "print(\"Idle files combined into '../data/combined data files/combined_idle_batches.csv'\")\n",
    "\n",
    "# Combine attack files with a batch size of 5 and save\n",
    "attack_combined_df = combine_in_batches(attack_files, False,\"../data/combined data files/combined_attack_batches.csv\", batch_size)\n",
    "print(\"Attack files combined into '../data/combined data files/combined_attack_batches.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4ddadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of idle_combined_df: (74770, 21)\n",
      "Size of attack_combined_df: (90725, 21)\n"
     ]
    }
   ],
   "source": [
    "# Print the sizes of the DataFrames\n",
    "print(f\"Size of idle_combined_df: {idle_combined_df.shape}\")\n",
    "print(f\"Size of attack_combined_df: {attack_combined_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ee75b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating batches...\n",
      "[==================================================] 100.00%\n",
      "\n",
      "Shuffling batches...\n",
      "\n",
      "Writing batches to CSV...\n",
      "[====----------------------------------------------] 8.10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 23.77%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 39.60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 55.33%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 72.32%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.22%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00%\n",
      "Shuffled data successfully written to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Function to generate a progress bar\n",
    "def progress_bar(current, total, bar_length=50):\n",
    "    progress = current / total\n",
    "    bar = '=' * int(progress * bar_length) + '-' * (bar_length - int(progress * bar_length))\n",
    "    sys.stdout.write(f'\\r[{bar}] {progress * 100:.2f}%')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Function to write shuffled batches directly to CSV\n",
    "def write_shuffled_batches_to_csv(idle_df, attack_df, batch_size, output_file):\n",
    "    # Calculate the maximum number of batches that can be drawn from each DataFrame\n",
    "    max_batches_idle = len(idle_df) // batch_size\n",
    "    max_batches_attack = len(attack_df) // batch_size\n",
    "    total_batches = min(max_batches_idle, max_batches_attack) * 2  # Equal distribution\n",
    "    \n",
    "    # Initialize batch counters\n",
    "    idle_start, attack_start = 0, 0\n",
    "    batches = []\n",
    "    \n",
    "    print(\"\\nCreating batches...\")\n",
    "    for i in range(total_batches // 2):\n",
    "        # Create one batch from idle data\n",
    "        idle_batch = idle_df.iloc[idle_start:idle_start + batch_size].copy()\n",
    "        idle_batch['Label'] = 0  # Label for idle\n",
    "        batches.append(idle_batch)\n",
    "        idle_start += batch_size\n",
    "        \n",
    "        # Create one batch from attack data\n",
    "        attack_batch = attack_df.iloc[attack_start:attack_start + batch_size].copy()\n",
    "        attack_batch['Label'] = 1  # Label for attack\n",
    "        batches.append(attack_batch)\n",
    "        attack_start += batch_size\n",
    "        \n",
    "        # Update progress bar for batch creation\n",
    "        progress_bar(i + 1, total_batches // 2)\n",
    "    \n",
    "    # Shuffle the batches\n",
    "    print(\"\\n\\nShuffling batches...\")\n",
    "    np.random.shuffle(batches)\n",
    "    \n",
    "    # Write the shuffled batches to CSV\n",
    "    print(\"\\nWriting batches to CSV...\")\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header row\n",
    "        writer.writerow(list(idle_df.columns) + ['Label'])\n",
    "        \n",
    "        for i, batch in enumerate(batches):\n",
    "            writer.writerows(batch.values)\n",
    "            \n",
    "            # Update progress bar for writing batches\n",
    "            progress_bar(i + 1, len(batches))\n",
    "    \n",
    "    print(\"\\nShuffled data successfully written to CSV.\")\n",
    "\n",
    "# Specify batch size and output file\n",
    "batch_size = 5\n",
    "output_file = \"../data/combined data files/shuffled_master_data.csv\"\n",
    "\n",
    "# Write shuffled batches directly to CSV\n",
    "write_shuffled_batches_to_csv(idle_combined_df, attack_combined_df, batch_size, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c43423d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idle_directory_bm = \"../data/combined data files/idle/benchmark data runs/\"\n",
    "attack_directory_bm = \"../data/combined data files/spectre/benchmark data runs/\"\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16789997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting for idle mode:\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 3000\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 973\n",
      "  BATCH1: 0\n",
      "  BATCH2: 575\n",
      "  BATCH3: 0\n",
      "  BATCH4: 392\n",
      "Valid Batches Included: 1060\n",
      "Idle files combined into '../data/combined data files/bm/combined_idle_batches.csv'\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 2842\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 414\n",
      "  BATCH1: 70\n",
      "  BATCH2: 555\n",
      "  BATCH3: 156\n",
      "  BATCH4: 293\n",
      "Valid Batches Included: 1354\n",
      "Attack files combined into '../data/combined data files/bm/combined_attack_batches.csv'\n",
      "\n",
      "Creating batches...\n",
      "[==================================================] 100.00%\n",
      "\n",
      "Shuffling batches...\n",
      "\n",
      "Writing batches to CSV...\n",
      "[==================================================] 100.00%\n",
      "Shuffled data successfully written to CSV.\n"
     ]
    }
   ],
   "source": [
    "idle = \"1. idle/\"\n",
    "idle_idle_files = [file for file in os.listdir(idle_directory_bm+idle) if file.endswith('.csv')]\n",
    "attack_idle_files = [file for file in os.listdir(attack_directory_bm+idle) if file.endswith('.csv')]\n",
    "\n",
    "idle_directory = idle_directory_bm+idle\n",
    "attack_directory = attack_directory_bm+idle\n",
    "print(\"starting for idle mode:\")\n",
    "# Combine idle files with a batch size of 5 and save\n",
    "idle_combined_df = combine_in_batches(idle_idle_files, True, \"../data/combined data files/bm/combined_idle_batches.csv\", batch_size)\n",
    "print(\"Idle files combined into '../data/combined data files/bm/combined_idle_batches.csv'\")\n",
    "\n",
    "# Combine attack files with a batch size of 5 and save\n",
    "attack_combined_df = combine_in_batches(attack_idle_files, False,\"../data/combined data files/bm/combined_attack_batches.csv\", batch_size)\n",
    "print(\"Attack files combined into '../data/combined data files/bm/combined_attack_batches.csv'\")\n",
    "\n",
    "output_file = \"../data/combined data files/bm/idle_shuffled_master_data.csv\"\n",
    "\n",
    "# Write shuffled batches directly to CSV\n",
    "write_shuffled_batches_to_csv(idle_combined_df, attack_combined_df, batch_size, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da207001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting for cpu mode:\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 3005\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 0\n",
      "  BATCH1: 0\n",
      "  BATCH2: 0\n",
      "  BATCH3: 0\n",
      "  BATCH4: 0\n",
      "Valid Batches Included: 3005\n",
      "Idle files combined into '../data/combined data files/bm/combined_cpu_idle_batches.csv'\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 3205\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 1032\n",
      "  BATCH1: 1113\n",
      "  BATCH2: 0\n",
      "  BATCH3: 1\n",
      "  BATCH4: 0\n",
      "Valid Batches Included: 1059\n",
      "Attack files combined into '../data/combined data files/bm/combined_cpu_attack_batches.csv'\n",
      "\n",
      "Creating batches...\n",
      "[==================================================] 100.00%\n",
      "\n",
      "Shuffling batches...\n",
      "\n",
      "Writing batches to CSV...\n",
      "[================================================--] 97.03%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode = \"2. cpu/\"\n",
    "idle_mode_files = [file for file in os.listdir(idle_directory_bm+mode) if file.endswith('.csv')]\n",
    "attack_mode_files = [file for file in os.listdir(attack_directory_bm+mode) if file.endswith('.csv')]\n",
    "\n",
    "idle_directory = idle_directory_bm+mode\n",
    "attack_directory = attack_directory_bm+mode\n",
    "print(\"starting for cpu mode:\")\n",
    "# Combine idle files with a batch size of 5 and save\n",
    "idle_combined_df = combine_in_batches(idle_mode_files, True, \"../data/combined data files/bm/combined_cpu_idle_batches.csv\", batch_size)\n",
    "print(\"Idle files combined into '../data/combined data files/bm/combined_cpu_idle_batches.csv'\")\n",
    "\n",
    "# Combine attack files with a batch size of 5 and save\n",
    "attack_combined_df = combine_in_batches(attack_mode_files, False,\"../data/combined data files/bm/combined_cpu_attack_batches.csv\", batch_size)\n",
    "print(\"Attack files combined into '../data/combined data files/bm/combined_cpu_attack_batches.csv'\")\n",
    "\n",
    "output_file = \"../data/combined data files/bm/cpu_shuffled_master_data.csv\"\n",
    "\n",
    "# Write shuffled batches directly to CSV\n",
    "write_shuffled_batches_to_csv(idle_combined_df, attack_combined_df, batch_size, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ceaf44f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting for memory mode:\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 2530\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 1\n",
      "  BATCH1: 1\n",
      "  BATCH2: 0\n",
      "  BATCH3: 0\n",
      "  BATCH4: 0\n",
      "Valid Batches Included: 2528\n",
      "Idle files combined into '../data/combined data files/bm/combined_memory_idle_batches.csv'\n",
      "Processing Files:\n",
      "[==================================================] 100.00%\n",
      "Analysis of Batches:\n",
      "Total Batches Processed: 2309\n",
      "Dropped Batches by Category:\n",
      "  DEFAULT: 223\n",
      "  BATCH1: 198\n",
      "  BATCH2: 11\n",
      "  BATCH3: 137\n",
      "  BATCH4: 0\n",
      "Valid Batches Included: 1740\n",
      "Attack files combined into '../data/combined data files/bm/combined_memory_attack_batches.csv'\n",
      "\n",
      "Creating batches...\n",
      "[==================================================] 100.00%\n",
      "\n",
      "Shuffling batches...\n",
      "\n",
      "Writing batches to CSV...\n",
      "[====================================--------------] 72.70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode = \"3. memory/\"\n",
    "idle_mode_files = [file for file in os.listdir(idle_directory_bm+mode) if file.endswith('.csv')]\n",
    "attack_mode_files = [file for file in os.listdir(attack_directory_bm+mode) if file.endswith('.csv')]\n",
    "\n",
    "idle_directory = idle_directory_bm+mode\n",
    "attack_directory = attack_directory_bm+mode\n",
    "print(\"starting for memory mode:\")\n",
    "# Combine idle files with a batch size of 5 and save\n",
    "idle_combined_df = combine_in_batches(idle_mode_files, True, \"../data/combined data files/bm/combined_memory_idle_batches.csv\", batch_size)\n",
    "print(\"Idle files combined into '../data/combined data files/bm/combined_memory_idle_batches.csv'\")\n",
    "\n",
    "# Combine attack files with a batch size of 5 and save\n",
    "attack_combined_df = combine_in_batches(attack_mode_files, False,\"../data/combined data files/bm/combined_memory_attack_batches.csv\", batch_size)\n",
    "print(\"Attack files combined into '../data/combined data files/bm/combined_memory_attack_batches.csv'\")\n",
    "\n",
    "output_file = \"../data/combined data files/bm/memory_shuffled_master_data.csv\"\n",
    "\n",
    "# Write shuffled batches directly to CSV\n",
    "write_shuffled_batches_to_csv(idle_combined_df, attack_combined_df, batch_size, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fce5ebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../data/combined data files/idle/benchmark data runs/4. cpu+memory/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. cpu+memory/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m idle_mode_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(idle_directory_bm\u001b[38;5;241m+\u001b[39mmode) \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      3\u001b[0m attack_mode_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(attack_directory_bm\u001b[38;5;241m+\u001b[39mmode) \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      5\u001b[0m idle_directory \u001b[38;5;241m=\u001b[39m idle_directory_bm\u001b[38;5;241m+\u001b[39mmode\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../data/combined data files/idle/benchmark data runs/4. cpu+memory/'"
     ]
    }
   ],
   "source": [
    "mode = \"4. cpu +memory/\"\n",
    "idle_mode_files = [file for file in os.listdir(idle_directory_bm+mode) if file.endswith('.csv')]\n",
    "attack_mode_files = [file for file in os.listdir(attack_directory_bm+mode) if file.endswith('.csv')]\n",
    "\n",
    "idle_directory = idle_directory_bm+mode\n",
    "attack_directory = attack_directory_bm+mode\n",
    "print(\"starting for combined mode:\")\n",
    "# Combine idle files with a batch size of 5 and save\n",
    "idle_combined_df = combine_in_batches(idle_mode_files, True, \"../data/combined data files/bm/combined_ccombined_idle_batches.csv\", batch_size)\n",
    "print(\"Idle files combined into '../data/combined data files/bm/combined_combined_idle_batches.csv'\")\n",
    "\n",
    "# Combine attack files with a batch size of 5 and save\n",
    "attack_combined_df = combine_in_batches(attack_mode_files, False,\"../data/combined data files/bm/combined_combined_attack_batches.csv\", batch_size)\n",
    "print(\"Attack files combined into '../data/combined data files/bm/combined_combined_attack_batches.csv'\")\n",
    "\n",
    "output_file = \"../data/combined data files/bm/combined_shuffled_master_data.csv\"\n",
    "\n",
    "# Write shuffled batches directly to CSV\n",
    "write_shuffled_batches_to_csv(idle_combined_df, attack_combined_df, batch_size, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c0b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
