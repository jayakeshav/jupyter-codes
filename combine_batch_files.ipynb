{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745961d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba976ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Spectre Files\n",
      "Folder '20241118_113051' has already been executed. Hence Skipping\n",
      "Folder '20241118_115238' has already been executed. Hence Skipping\n",
      "Folder '20241118_120120' has already been executed. Hence Skipping\n",
      "Folder '20241118_121027' has already been executed. Hence Skipping\n",
      "Folder '20241118_121950' has already been executed. Hence Skipping\n",
      "Folder '20241118_122811' has already been executed. Hence Skipping\n",
      "Folder '20241209_094459' has already been executed. Hence Skipping\n",
      "Folder '20241209_095248' has already been executed. Hence Skipping\n",
      "Folder '20241209_100052' has already been executed. Hence Skipping\n",
      "Folder '20241209_100905' has already been executed. Hence Skipping\n",
      "Folder '20241209_101819' has already been executed. Hence Skipping\n",
      "Folder '20241209_102645' has already been executed. Hence Skipping\n",
      "Folder '20241209_103529' has already been executed. Hence Skipping\n",
      "Folder '20241209_104449' has already been executed. Hence Skipping\n",
      "Folder '20241209_105333' has already been executed. Hence Skipping\n",
      "Folder '20241209_110229' has already been executed. Hence Skipping\n",
      "Folder '20241209_111159' has already been executed. Hence Skipping\n",
      "Folder '20241209_112034' has already been executed. Hence Skipping\n",
      "Folder '20241209_112831' has already been executed. Hence Skipping\n",
      "Folder '20241209_113629' has already been executed. Hence Skipping\n",
      "Folder '20241209_114505' has already been executed. Hence Skipping\n",
      "Folder '20241209_115346' has already been executed. Hence Skipping\n",
      "Folder '20241209_120326' has already been executed. Hence Skipping\n",
      "Folder '20241209_121202' has already been executed. Hence Skipping\n",
      "Folder '20241209_122010' has already been executed. Hence Skipping\n",
      "Folder '20241209_122910' has already been executed. Hence Skipping\n",
      "Folder '20241209_123753' has already been executed. Hence Skipping\n",
      "Folder '20241209_124649' has already been executed. Hence Skipping\n",
      "Folder '20241209_125606' has already been executed. Hence Skipping\n",
      "Folder '20241209_130529' has already been executed. Hence Skipping\n",
      "Folder '20241209_131420' has already been executed. Hence Skipping\n",
      "processing Idle Files\n",
      "Folder '20241118_113051' has already been executed. Hence Skipping\n",
      "Folder '20241118_115238' has already been executed. Hence Skipping\n",
      "Folder '20241118_120120' has already been executed. Hence Skipping\n",
      "Folder '20241118_121027' has already been executed. Hence Skipping\n",
      "Folder '20241118_121950' has already been executed. Hence Skipping\n",
      "Folder '20241118_122811' has already been executed. Hence Skipping\n",
      "Folder '20241209_094459' has already been executed. Hence Skipping\n",
      "Folder '20241209_095248' has already been executed. Hence Skipping\n",
      "Folder '20241209_100052' has already been executed. Hence Skipping\n",
      "Folder '20241209_100905' has already been executed. Hence Skipping\n",
      "Folder '20241209_101819' has already been executed. Hence Skipping\n",
      "Folder '20241209_102645' has already been executed. Hence Skipping\n",
      "Folder '20241209_103529' has already been executed. Hence Skipping\n",
      "Folder '20241209_104449' has already been executed. Hence Skipping\n",
      "Folder '20241209_105333' has already been executed. Hence Skipping\n",
      "Folder '20241209_110229' has already been executed. Hence Skipping\n",
      "Folder '20241209_111159' has already been executed. Hence Skipping\n",
      "Folder '20241209_112034' has already been executed. Hence Skipping\n",
      "Folder '20241209_112831' has already been executed. Hence Skipping\n",
      "Folder '20241209_113629' has already been executed. Hence Skipping\n",
      "Folder '20241209_114505' has already been executed. Hence Skipping\n",
      "Folder '20241209_115346' has already been executed. Hence Skipping\n",
      "Folder '20241209_120326' has already been executed. Hence Skipping\n",
      "Folder '20241209_121202' has already been executed. Hence Skipping\n",
      "Folder '20241209_122010' has already been executed. Hence Skipping\n",
      "Folder '20241209_122910' has already been executed. Hence Skipping\n",
      "Folder '20241209_123753' has already been executed. Hence Skipping\n",
      "Folder '20241209_124649' has already been executed. Hence Skipping\n",
      "Folder '20241209_125606' has already been executed. Hence Skipping\n",
      "Folder '20241209_130529' has already been executed. Hence Skipping\n",
      "Folder '20241209_131420' has already been executed. Hence Skipping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idle_file_directory = \"../data/idle/\"\n",
    "attack_file_directory = \"../data/spectre/\"\n",
    "\n",
    "combined_directory = \"../data/combined data files/\"\n",
    "\n",
    "idle_run_folders = [d for d in os.listdir(idle_file_directory) if os.path.isdir(os.path.join(idle_file_directory, d))]\n",
    "attack_run_folders = [d for d in os.listdir(attack_file_directory) if os.path.isdir(os.path.join(attack_file_directory, d))]\n",
    "\n",
    "\n",
    "\n",
    "def getBatchFileInDirectory(path,folder):\n",
    "    finalpath = os.path.join(path,folder,\"csv\")\n",
    "    if os.path.isdir(finalpath):\n",
    "        # Get the list of files in the final path\n",
    "        batch_files = [f for f in os.listdir(finalpath) if os.path.isfile(os.path.join(finalpath, f))]\n",
    "        return batch_files\n",
    "    else:\n",
    "        # Return an empty list if the path does not exist or is not a directory\n",
    "        return []\n",
    "\n",
    "def getExecutedFolders(file_path):\n",
    "    executed_folders = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read each line and strip any whitespace or newline characters\n",
    "            executed_folders = [line.strip() for line in file if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except IOError:\n",
    "        print(f\"An error occurred while reading the file {file_path}.\")\n",
    "    return executed_folders\n",
    "    \n",
    "def getValidFolders(folders,isIdle):\n",
    "    valid_folders = []\n",
    "    executed_folders_path = (idle_file_directory if isIdle else attack_file_directory) + \"executed_folders.txt\"\n",
    "    executed_folders = getExecutedFolders(executed_folders_path)\n",
    "    for folder in folders:\n",
    "        if folder in executed_folders:\n",
    "            print(f\"Folder '{folder}' has already been executed. Hence Skipping\")\n",
    "        else:\n",
    "            print(f\"Folder '{folder}' has not been executed. Hence Vaild\")\n",
    "            valid_folders.append(folder)\n",
    "    \n",
    "    return valid_folders\n",
    "\n",
    "\n",
    "def getValidBatchFiles(isIdle):\n",
    "    # Determine folders and path based on isIdle\n",
    "    folders = idle_run_folders if isIdle else attack_run_folders\n",
    "    path = idle_file_directory if isIdle else attack_file_directory\n",
    "\n",
    "    # Get the valid folders that haven't been executed\n",
    "    valid_folders = getValidFolders(folders, isIdle)\n",
    "\n",
    "    # Initialize a list to store batch paths and files as key-value pairs\n",
    "    validBatchFiles = []\n",
    "\n",
    "    # Iterate over each valid folder\n",
    "    for folder in valid_folders:\n",
    "        # Get batch files and batch path\n",
    "        batch_files = getBatchFileInDirectory(path, folder)\n",
    "        batchPath = os.path.join(path, folder, \"csv\")\n",
    "\n",
    "        # Store batch path and files as a dictionary in validBatchFiles\n",
    "        validBatchFiles.append({batchPath: batch_files})\n",
    "\n",
    "    return validBatchFiles\n",
    "\n",
    "def validBatchPath(batchPath, batch_files):\n",
    "    # Generate full paths by joining batchPath with each file name in batch_files\n",
    "    complete_paths = [os.path.join(batchPath, file) for file in batch_files]\n",
    "    return complete_paths\n",
    "\n",
    "def loadBatchFilesAsDataFrames(batchPath, batch_files):\n",
    "    batch_dataframes = {}\n",
    "    batch_count = 1  # Counter to label batch files sequentially\n",
    "\n",
    "    for file in batch_files:\n",
    "        file_path = os.path.join(batchPath, file)\n",
    "        \n",
    "        # Check if the file is a default file\n",
    "        if \"default\" in file:\n",
    "            key = \"default\"\n",
    "        # Check if the file is a batch file and assign a batch key sequentially\n",
    "        elif \"batch\" in file:\n",
    "            key = f\"batch_{batch_count}\"\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            # Skip files that do not match the expected naming convention\n",
    "            continue\n",
    "\n",
    "        # Load the CSV file as a DataFrame and add it to the dictionary\n",
    "        batch_dataframes[key] = pd.read_csv(file_path)\n",
    "\n",
    "    return batch_dataframes\n",
    "\n",
    "def cleanupBatchFile(df):\n",
    "    # Step 1: Drop columns where all values are zero\n",
    "    df = df.loc[:, (df != 0).any(axis=0)].copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Step 2: Reset the index column to start at 0 and retain increments\n",
    "    index_column_name = df.columns[0]\n",
    "    df.loc[:, index_column_name] = df[index_column_name] - df[index_column_name].iloc[0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def loadAndCleanBatchFiles(batchPath, batch_files):\n",
    "    # Load the batch files using the provided function\n",
    "    batch_dataframes = loadBatchFilesAsDataFrames(batchPath, batch_files)\n",
    "    \n",
    "    # Dictionary to store cleaned DataFrames\n",
    "    cleaned_up_batch_dataframes = {}\n",
    "    \n",
    "    # Apply cleaning to each loaded DataFrame\n",
    "    for key, df in batch_dataframes.items():\n",
    "        cleaned_up_batch_dataframes[key] = cleanupBatchFile(df)\n",
    "    \n",
    "    return cleaned_up_batch_dataframes\n",
    "\n",
    "def generateMasterDataFrame(cleaned_up_batch_dataframes):\n",
    "    # Merge all DataFrames on the 'index' column using an outer join to keep all rows\n",
    "    dataframes = list(cleaned_up_batch_dataframes.values())\n",
    "    master_df = reduce(lambda left, right: pd.merge(left, right, on='index', how='outer'), dataframes)\n",
    "    \n",
    "    # Sort by 'index' column and reset the DataFrame index\n",
    "    master_df.sort_values(by='index', inplace=True)\n",
    "    master_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    master_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Convert all columns to integers\n",
    "    master_df = master_df.astype(int)\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def saveMasterDataFrame(master_df, attack_type, filename):\n",
    "    # Create the output path\n",
    "    output_path = os.path.join(combined_directory, attack_type, f\"{filename}.csv\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create directories if not exist\n",
    "    \n",
    "    # Save the master DataFrame to the specified CSV location\n",
    "    master_df.to_csv(output_path, index=False)\n",
    "    print(f\"Master DataFrame saved to {output_path}\")\n",
    "    \n",
    "def addFolderNameToExecutedFolders(executed_folders_path, folder_name):\n",
    "    with open(executed_folders_path, 'a') as file:\n",
    "        file.write(f\"{folder_name}\\n\")\n",
    "    print(f\"Added '{folder_name}' to executed_folders.txt\")\n",
    "    \n",
    "def processBatches(isIdle):\n",
    "    valid_batch_files = getValidBatchFiles(isIdle)\n",
    "\n",
    "    for batch_dict in valid_batch_files:\n",
    "        for batchPath, batch_files in batch_dict.items():\n",
    "\n",
    "            parent_path = os.path.dirname(batchPath)\n",
    "            # Extract the folder name\n",
    "            folder_name = os.path.basename(parent_path)\n",
    "\n",
    "            attackType = \"Idle\" if isIdle else \"Spectre\"\n",
    "\n",
    "            print(f\"Proxessing Folder: \",folder_name,\" for AttackType: \",attackType)\n",
    "            clean_batches = loadAndCleanBatchFiles(batchPath, batch_files)\n",
    "            master_df = generateMasterDataFrame(clean_batches)\n",
    "\n",
    "            fileName = attackType+\"_\" + folder_name\n",
    "            saveMasterDataFrame(master_df,attackType,fileName)\n",
    "            print(f\"Proxessing Folder: \",folder_name,\" for AttackType: \",attackType,\"Completed Sucessfully\")\n",
    "            executed_folders_path = (idle_file_directory if isIdle else attack_file_directory) + \"executed_folders.txt\"\n",
    "            addFolderNameToExecutedFolders(executed_folders_path, folder_name)\n",
    "\n",
    "            \n",
    "isIdle = False\n",
    "print(\"processing Spectre Files\")\n",
    "processBatches(isIdle)\n",
    "\n",
    "isIdle = True\n",
    "\n",
    "\n",
    "print(\"processing Idle Files\")\n",
    "processBatches(isIdle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a1f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab9cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
